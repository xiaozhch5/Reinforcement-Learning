{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\\nonumber\n",
    "强化学习基础\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本篇文章主要参考：http://www.atyun.com/10331.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward and return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在强化学习过程中，agent的目的就是为了最大化累计未来的回报，我们用未来的累计折现回报表示（cumulative discounted reward）:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots = \\sum\\limits_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$，其中$0 < \\gamma < 1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果γ等于1，这个方程就变成了对所有的回报都同样的关心，无论在什么时候。另一方面，当γ等于0时，我们只关心眼前的回报，而不关心以后的回报。这将导致我们的算法极其短视。它将学会采取目前最好的行动，但不会考虑行动对未来的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个策略，写成$\\pi(s, a)$，描述了一种行动方式。表示在某个状态下采取某个动作的概率，一次对于一个状态，有："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "\\sum\\limits_{a}\\pi(s, a) = 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面的例子中，当我们饥饿的时候，我们可以在两种行为之间做出选择，要么吃，要么不吃。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](3-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的策略应该是描述每个状态中采取形动，所以一个等概率的随机策略就像："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "\\pi(\\mathrm{hungary, E})=0.5 \\\\\n",
    "\\pi(\\mathrm{humgary, \\bar{E}}) = 0.5 \\\\\n",
    "\\pi(\\mathrm{full, \\bar{E}}) = 1.0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里$\\mathrm{E}$是行为“吃”，$\\mathrm{\\bar{E}}$是行为“不吃”。上述策略表示，当你处于饥饿状态时候，选择“吃”或者“不吃”的概率是相同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在强化学习中就是要找到一个最优策略，使得return最大，定义为：$\\pi^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优策略告诉我们如何采取形动来最大化每个状态的返回。因为这是一个很简单的例子，所以很容易看出在这种情况下最优策略是在饥饿时总是“吃”。也就是:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "\\pi^*(\\mathrm{humgary, E}) = 1.0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，最优策略是确定的，每个状态都有一个最优的动作，有时候被写为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "\\pi^*(s) = a\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它是状态中最优动作的映射。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了学习得到最优策略，引入了值函数这个概念。在强化学习中有两种类型的值函数：状态值函数（state value function），用V(s)表示；和状态动作值函数（action value function），用Q（s， a）表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态值函数在遵循策略时描述一个状态的值。在某个状态下的预期返还为：(期望)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "V^\\pi(s) = \\mathrm{E}_\\pi [R_t| s_t=s]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义$V^*(s)$表示$V^\\pi(s)$的最大可能取值，其中策略$\\pi$是可变的。即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "V^*(s) = \\max_\\pi V^\\pi(s)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每个状态下，可以得到的最佳值的策略称为最佳策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定状态$s$, 动作$a$和策略$\\pi$，在策略$\\pi$下的状态动作对的动作值函数可以表示为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "Q^\\pi(s, a) = \\mathrm{E}_\\pi [R_t|s_t = s, a_t = a]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据马尔科夫决策理论，假设$\\pi^*$为最优策略，那么我们根据状态$s$下$Q^{\\pi^*}(s, :)$(也称作最佳动作值函数$Q^*$)最大值来选择对应的动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation ---- 用于计算 action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说到贝尔曼方程之前，先来看几个定义："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "P_{ss}^a = Pr(s_{t+1} = s'|s_t = s, a_t = a)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_{ss'}^a$为过度概率，表示在状态$s$下，采取行动$a$之后转移到状态$s'$的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这边给出动作值函数的推导过程：（状态值函数的推导类似）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](14.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration and Value iteration --- 如何将值函数和策略结合并互相优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](value.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
