{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本篇文章主要参考：http://www.atyun.com/10331.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reward and return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在强化学习过程中，agent的目的就是为了最大化累计未来的回报，我们用未来的累计折现回报表示（cumulative discounted reward）:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots = \\sum\\limits_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$，其中$0 < \\gamma < 1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果γ等于1，这个方程就变成了对所有的回报都同样的关心，无论在什么时候。另一方面，当γ等于0时，我们只关心眼前的回报，而不关心以后的回报。这将导致我们的算法极其短视。它将学会采取目前最好的行动，但不会考虑行动对未来的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个策略，写成$\\pi(s, a)$，描述了一种行动方式。表示在某个状态下采取某个动作的概率，一次对于一个状态，有："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "\\sum\\limits_{a}\\pi(s, a) = 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面的例子中，当我们饥饿的时候，我们可以在两种行为之间做出选择，要么吃，要么不吃。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](3-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的策略应该是描述每个状态中采取形动，所以一个等概率的随机策略就像："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "\\pi(\\mathrm{hungary, E})=0.5 \\\\\n",
    "\\pi(\\mathrm{humgary, \\bar{E}}) = 0.5 \\\\\n",
    "\\pi(\\mathrm{full, \\bar{E}}) = 1.0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里$\\mathrm{E}$是行为“吃”，$\\mathrm{\\bar{E}}$是行为“不吃”。上述策略表示，当你处于饥饿状态时候，选择“吃”或者“不吃”的概率是相同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在强化学习中就是要找到一个最优策略，使得return最大，定义为：$\\pi^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优策略告诉我们如何采取形动来最大化每个状态的返回。因为这是一个很简单的例子，所以很容易看出在这种情况下最优策略是在饥饿时总是“吃”。也就是:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "\\pi^*(\\mathrm{humgary, E}) = 1.0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，最优策略是确定的，每个状态都有一个最优的动作，有时候被写为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "\\pi^*(s) = a\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它是状态中最优动作的映射。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了学习得到最优策略，引入了值函数这个概念。在强化学习中有两种类型的值函数：状态值函数（state value function），用V(s)表示；和状态动作值函数（action value function），用Q（s， a）表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态值函数在遵循策略时描述一个状态的值，当从状态的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
